*This files contains parameters found with hyperparameter tuning in experiments presented in the paper :*
`(to appear) Alya Itani, Laurent Brisson, Serge Garlatti, Understanding Learner's Drop-out in MOOCs, In, 19th International Conference on Intelligent Data Engineering and Automated Learning, Madrid,2018`

# HTML Course
## 50% of the activities

Gradient Boosting
{'learning_rate': 0.15000000000000002,
 'max_depth': 5,
 'max_features': 19,
 'min_samples_leaf': 60,
 'min_samples_split': 1400,
 'n_estimators': 50,
 'random_state': 10,
 'subsample': 0.9}

Random Forest
{'criterion': 'gini',
 'max_features': 17,
 'min_samples_leaf': 31,
 'n_estimators': 601,
 'n_jobs': 4,
 'oob_score': True,
 'random_state': 42}

Decision Tree
{'criterion': 'gini',
 'max_depth': 7,
 'max_features': 17,
 'min_samples_leaf': 21,
 'min_samples_split': 600}

Logistic Regression
{'C': 10,
 'class_weight': 'balanced',
 'dual': False,
 'fit_intercept': True,
 'penalty': 'l2',
 'tol': 1e-06}

K Nearest Neighbors
{'n_neighbors': 31, 'p': 1, 'weights': 'distance'}

# HTML Course
## 25% of the activities

Gradient Boosting
{'learning_rate': 0.15000000000000002,
 'max_depth': 15,
 'max_features': 17,
 'min_samples_leaf': 50,
 'min_samples_split': 2000,
 'n_estimators': 80,
 'random_state': 10,
 'subsample': 0.9}

Random Forest
{'criterion': 'entropy',
 'max_features': 13,
 'min_samples_leaf': 11,
 'n_estimators': 601,
 'n_jobs': 4,
 'oob_score': True,
 'random_state': 42}

Decision Tree
{'criterion': 'gini',
 'max_depth': 7,
 'max_features': 15,
 'min_samples_leaf': 31,
 'min_samples_split': 1400}

Logistic Regression
{'C': 1000,
 'class_weight': 'balanced',
 'dual': False,
 'fit_intercept': True,
 'penalty': 'l2',
 'tol': 1e-05}

K Nearest Neighbors
{'n_neighbors': 941, 'p': 2, 'weights': 'distance'}

# Web course
## 50% of the activities

Gradient Boosting
{'learning_rate': 0.10000000000000001,
 'max_depth': 5,
 'max_features': 5,
 'min_samples_leaf': 30,
 'min_samples_split': 600,
 'n_estimators': 20,
 'random_state': 10,
 'subsample': 1}

Random Forest
{'criterion': 'entropy',
 'max_features': 17,
 'min_samples_leaf': 11,
 'n_estimators': 601,
 'n_jobs': 4,
 'oob_score': True,
 'random_state': 10}

Decision Tree
{'criterion': 'entropy',
 'max_depth': 11,
 'max_features': 13,
 'min_samples_leaf': 21,
 'min_samples_split': 200}

Logistic Regression
{'C': 100,
 'class_weight': 'balanced',
 'dual': False,
 'fit_intercept': True,
 'penalty': 'l2',
 'tol': 0.0001}

K Nearest Neighbors
{'n_neighbors': 112, 'p': 2, 'weights': 'distance'}

# Web course
## 25% of the activities

Gradient Boosting
{'learning_rate': 0.20000000000000001,
 'max_depth': 7,
 'max_features': 11,
 'min_samples_leaf': 50,
 'min_samples_split': 200,
 'n_estimators': 80,
 'random_state': 10,
 'subsample': 0.9}

Random Forest
{'criterion': 'gini',
 'max_features': 5,
 'min_samples_leaf': 1,
 'n_estimators': 201,
 'n_jobs': 4,
 'oob_score': True,
 'random_state': 42}

Decision Tree
{'criterion': 'gini',
 'max_depth': 7,
 'max_features': 13,
 'min_samples_leaf': 1,
 'min_samples_split': 200}

Logistic Regression
{'C': 100,
 'class_weight': 'balanced',
 'dual': False,
 'fit_intercept': True,
 'penalty': 'l2',
 'tol': 0.0001}

K Nearest Neighbors
{'n_neighbors': 72, 'p': 3, 'weights': 'distance'}
